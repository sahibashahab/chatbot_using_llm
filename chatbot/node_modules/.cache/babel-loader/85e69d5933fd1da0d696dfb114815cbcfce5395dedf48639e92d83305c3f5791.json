{"ast":null,"code":"// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nimport { APIResource } from \"../../resource.mjs\";\nimport { isRequestOptions } from \"../../core.mjs\";\nimport * as RunsAPI from \"./runs/runs.mjs\";\nimport { RunListResponsesPage, Runs } from \"./runs/runs.mjs\";\nimport { CursorPage } from \"../../pagination.mjs\";\nexport class Evals extends APIResource {\n  constructor() {\n    super(...arguments);\n    this.runs = new RunsAPI.Runs(this._client);\n  }\n  /**\n   * Create the structure of an evaluation that can be used to test a model's\n   * performance. An evaluation is a set of testing criteria and a datasource. After\n   * creating an evaluation, you can run it on different models and model parameters.\n   * We support several types of graders and datasources. For more information, see\n   * the [Evals guide](https://platform.openai.com/docs/guides/evals).\n   */\n  create(body, options) {\n    return this._client.post('/evals', {\n      body,\n      ...options\n    });\n  }\n  /**\n   * Get an evaluation by ID.\n   */\n  retrieve(evalId, options) {\n    return this._client.get(`/evals/${evalId}`, options);\n  }\n  /**\n   * Update certain properties of an evaluation.\n   */\n  update(evalId, body, options) {\n    return this._client.post(`/evals/${evalId}`, {\n      body,\n      ...options\n    });\n  }\n  list(query = {}, options) {\n    if (isRequestOptions(query)) {\n      return this.list({}, query);\n    }\n    return this._client.getAPIList('/evals', EvalListResponsesPage, {\n      query,\n      ...options\n    });\n  }\n  /**\n   * Delete an evaluation.\n   */\n  del(evalId, options) {\n    return this._client.delete(`/evals/${evalId}`, options);\n  }\n}\nexport class EvalListResponsesPage extends CursorPage {}\nEvals.EvalListResponsesPage = EvalListResponsesPage;\nEvals.Runs = Runs;\nEvals.RunListResponsesPage = RunListResponsesPage;","map":{"version":3,"names":["APIResource","isRequestOptions","RunsAPI","RunListResponsesPage","Runs","CursorPage","Evals","constructor","runs","_client","create","body","options","post","retrieve","evalId","get","update","list","query","getAPIList","EvalListResponsesPage","del","delete"],"sources":["D:\\office\\react\\react\\chatbot\\node_modules\\openai\\src\\resources\\evals\\evals.ts"],"sourcesContent":["// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../resource';\nimport { isRequestOptions } from '../../core';\nimport * as Core from '../../core';\nimport * as Shared from '../shared';\nimport * as ResponsesAPI from '../responses/responses';\nimport * as RunsAPI from './runs/runs';\nimport {\n  CreateEvalCompletionsRunDataSource,\n  CreateEvalJSONLRunDataSource,\n  EvalAPIError,\n  RunCancelResponse,\n  RunCreateParams,\n  RunCreateResponse,\n  RunDeleteResponse,\n  RunListParams,\n  RunListResponse,\n  RunListResponsesPage,\n  RunRetrieveResponse,\n  Runs,\n} from './runs/runs';\nimport { CursorPage, type CursorPageParams } from '../../pagination';\n\nexport class Evals extends APIResource {\n  runs: RunsAPI.Runs = new RunsAPI.Runs(this._client);\n\n  /**\n   * Create the structure of an evaluation that can be used to test a model's\n   * performance. An evaluation is a set of testing criteria and a datasource. After\n   * creating an evaluation, you can run it on different models and model parameters.\n   * We support several types of graders and datasources. For more information, see\n   * the [Evals guide](https://platform.openai.com/docs/guides/evals).\n   */\n  create(body: EvalCreateParams, options?: Core.RequestOptions): Core.APIPromise<EvalCreateResponse> {\n    return this._client.post('/evals', { body, ...options });\n  }\n\n  /**\n   * Get an evaluation by ID.\n   */\n  retrieve(evalId: string, options?: Core.RequestOptions): Core.APIPromise<EvalRetrieveResponse> {\n    return this._client.get(`/evals/${evalId}`, options);\n  }\n\n  /**\n   * Update certain properties of an evaluation.\n   */\n  update(\n    evalId: string,\n    body: EvalUpdateParams,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<EvalUpdateResponse> {\n    return this._client.post(`/evals/${evalId}`, { body, ...options });\n  }\n\n  /**\n   * List evaluations for a project.\n   */\n  list(\n    query?: EvalListParams,\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<EvalListResponsesPage, EvalListResponse>;\n  list(options?: Core.RequestOptions): Core.PagePromise<EvalListResponsesPage, EvalListResponse>;\n  list(\n    query: EvalListParams | Core.RequestOptions = {},\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<EvalListResponsesPage, EvalListResponse> {\n    if (isRequestOptions(query)) {\n      return this.list({}, query);\n    }\n    return this._client.getAPIList('/evals', EvalListResponsesPage, { query, ...options });\n  }\n\n  /**\n   * Delete an evaluation.\n   */\n  del(evalId: string, options?: Core.RequestOptions): Core.APIPromise<EvalDeleteResponse> {\n    return this._client.delete(`/evals/${evalId}`, options);\n  }\n}\n\nexport class EvalListResponsesPage extends CursorPage<EvalListResponse> {}\n\n/**\n * A CustomDataSourceConfig which specifies the schema of your `item` and\n * optionally `sample` namespaces. The response schema defines the shape of the\n * data that will be:\n *\n * - Used to define your testing criteria and\n * - What data is required when creating a run\n */\nexport interface EvalCustomDataSourceConfig {\n  /**\n   * The json schema for the run data source items. Learn how to build JSON schemas\n   * [here](https://json-schema.org/).\n   */\n  schema: Record<string, unknown>;\n\n  /**\n   * The type of data source. Always `custom`.\n   */\n  type: 'custom';\n}\n\n/**\n * A LabelModelGrader object which uses a model to assign labels to each item in\n * the evaluation.\n */\nexport interface EvalLabelModelGrader {\n  input: Array<EvalLabelModelGrader.Input>;\n\n  /**\n   * The labels to assign to each item in the evaluation.\n   */\n  labels: Array<string>;\n\n  /**\n   * The model to use for the evaluation. Must support structured outputs.\n   */\n  model: string;\n\n  /**\n   * The name of the grader.\n   */\n  name: string;\n\n  /**\n   * The labels that indicate a passing result. Must be a subset of labels.\n   */\n  passing_labels: Array<string>;\n\n  /**\n   * The object type, which is always `label_model`.\n   */\n  type: 'label_model';\n}\n\nexport namespace EvalLabelModelGrader {\n  /**\n   * A message input to the model with a role indicating instruction following\n   * hierarchy. Instructions given with the `developer` or `system` role take\n   * precedence over instructions given with the `user` role. Messages with the\n   * `assistant` role are presumed to have been generated by the model in previous\n   * interactions.\n   */\n  export interface Input {\n    /**\n     * Text inputs to the model - can contain template strings.\n     */\n    content: string | ResponsesAPI.ResponseInputText | Input.OutputText;\n\n    /**\n     * The role of the message input. One of `user`, `assistant`, `system`, or\n     * `developer`.\n     */\n    role: 'user' | 'assistant' | 'system' | 'developer';\n\n    /**\n     * The type of the message input. Always `message`.\n     */\n    type?: 'message';\n  }\n\n  export namespace Input {\n    /**\n     * A text output from the model.\n     */\n    export interface OutputText {\n      /**\n       * The text output from the model.\n       */\n      text: string;\n\n      /**\n       * The type of the output text. Always `output_text`.\n       */\n      type: 'output_text';\n    }\n  }\n}\n\n/**\n * A StoredCompletionsDataSourceConfig which specifies the metadata property of\n * your stored completions query. This is usually metadata like `usecase=chatbot`\n * or `prompt-version=v2`, etc. The schema returned by this data source config is\n * used to defined what variables are available in your evals. `item` and `sample`\n * are both defined when using this data source config.\n */\nexport interface EvalStoredCompletionsDataSourceConfig {\n  /**\n   * The json schema for the run data source items. Learn how to build JSON schemas\n   * [here](https://json-schema.org/).\n   */\n  schema: Record<string, unknown>;\n\n  /**\n   * The type of data source. Always `stored_completions`.\n   */\n  type: 'stored_completions';\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n}\n\n/**\n * A StringCheckGrader object that performs a string comparison between input and\n * reference using a specified operation.\n */\nexport interface EvalStringCheckGrader {\n  /**\n   * The input text. This may include template strings.\n   */\n  input: string;\n\n  /**\n   * The name of the grader.\n   */\n  name: string;\n\n  /**\n   * The string check operation to perform. One of `eq`, `ne`, `like`, or `ilike`.\n   */\n  operation: 'eq' | 'ne' | 'like' | 'ilike';\n\n  /**\n   * The reference text. This may include template strings.\n   */\n  reference: string;\n\n  /**\n   * The object type, which is always `string_check`.\n   */\n  type: 'string_check';\n}\n\n/**\n * A TextSimilarityGrader object which grades text based on similarity metrics.\n */\nexport interface EvalTextSimilarityGrader {\n  /**\n   * The evaluation metric to use. One of `fuzzy_match`, `bleu`, `gleu`, `meteor`,\n   * `rouge_1`, `rouge_2`, `rouge_3`, `rouge_4`, `rouge_5`, or `rouge_l`.\n   */\n  evaluation_metric:\n    | 'fuzzy_match'\n    | 'bleu'\n    | 'gleu'\n    | 'meteor'\n    | 'rouge_1'\n    | 'rouge_2'\n    | 'rouge_3'\n    | 'rouge_4'\n    | 'rouge_5'\n    | 'rouge_l';\n\n  /**\n   * The text being graded.\n   */\n  input: string;\n\n  /**\n   * A float score where a value greater than or equal indicates a passing grade.\n   */\n  pass_threshold: number;\n\n  /**\n   * The text being graded against.\n   */\n  reference: string;\n\n  /**\n   * The type of grader.\n   */\n  type: 'text_similarity';\n\n  /**\n   * The name of the grader.\n   */\n  name?: string;\n}\n\n/**\n * An Eval object with a data source config and testing criteria. An Eval\n * represents a task to be done for your LLM integration. Like:\n *\n * - Improve the quality of my chatbot\n * - See how well my chatbot handles customer support\n * - Check if o3-mini is better at my usecase than gpt-4o\n */\nexport interface EvalCreateResponse {\n  /**\n   * Unique identifier for the evaluation.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the eval was created.\n   */\n  created_at: number;\n\n  /**\n   * Configuration of data sources used in runs of the evaluation.\n   */\n  data_source_config: EvalCustomDataSourceConfig | EvalStoredCompletionsDataSourceConfig;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * The name of the evaluation.\n   */\n  name: string;\n\n  /**\n   * The object type.\n   */\n  object: 'eval';\n\n  /**\n   * A list of testing criteria.\n   */\n  testing_criteria: Array<\n    | EvalLabelModelGrader\n    | EvalStringCheckGrader\n    | EvalTextSimilarityGrader\n    | EvalCreateResponse.Python\n    | EvalCreateResponse.ScoreModel\n  >;\n}\n\nexport namespace EvalCreateResponse {\n  /**\n   * A PythonGrader object that runs a python script on the input.\n   */\n  export interface Python {\n    /**\n     * The name of the grader.\n     */\n    name: string;\n\n    /**\n     * The source code of the python script.\n     */\n    source: string;\n\n    /**\n     * The object type, which is always `python`.\n     */\n    type: 'python';\n\n    /**\n     * The image tag to use for the python script.\n     */\n    image_tag?: string;\n\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n  }\n\n  /**\n   * A ScoreModelGrader object that uses a model to assign a score to the input.\n   */\n  export interface ScoreModel {\n    /**\n     * The input text. This may include template strings.\n     */\n    input: Array<ScoreModel.Input>;\n\n    /**\n     * The model to use for the evaluation.\n     */\n    model: string;\n\n    /**\n     * The name of the grader.\n     */\n    name: string;\n\n    /**\n     * The object type, which is always `score_model`.\n     */\n    type: 'score_model';\n\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n\n    /**\n     * The range of the score. Defaults to `[0, 1]`.\n     */\n    range?: Array<number>;\n\n    /**\n     * The sampling parameters for the model.\n     */\n    sampling_params?: unknown;\n  }\n\n  export namespace ScoreModel {\n    /**\n     * A message input to the model with a role indicating instruction following\n     * hierarchy. Instructions given with the `developer` or `system` role take\n     * precedence over instructions given with the `user` role. Messages with the\n     * `assistant` role are presumed to have been generated by the model in previous\n     * interactions.\n     */\n    export interface Input {\n      /**\n       * Text inputs to the model - can contain template strings.\n       */\n      content: string | ResponsesAPI.ResponseInputText | Input.OutputText;\n\n      /**\n       * The role of the message input. One of `user`, `assistant`, `system`, or\n       * `developer`.\n       */\n      role: 'user' | 'assistant' | 'system' | 'developer';\n\n      /**\n       * The type of the message input. Always `message`.\n       */\n      type?: 'message';\n    }\n\n    export namespace Input {\n      /**\n       * A text output from the model.\n       */\n      export interface OutputText {\n        /**\n         * The text output from the model.\n         */\n        text: string;\n\n        /**\n         * The type of the output text. Always `output_text`.\n         */\n        type: 'output_text';\n      }\n    }\n  }\n}\n\n/**\n * An Eval object with a data source config and testing criteria. An Eval\n * represents a task to be done for your LLM integration. Like:\n *\n * - Improve the quality of my chatbot\n * - See how well my chatbot handles customer support\n * - Check if o3-mini is better at my usecase than gpt-4o\n */\nexport interface EvalRetrieveResponse {\n  /**\n   * Unique identifier for the evaluation.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the eval was created.\n   */\n  created_at: number;\n\n  /**\n   * Configuration of data sources used in runs of the evaluation.\n   */\n  data_source_config: EvalCustomDataSourceConfig | EvalStoredCompletionsDataSourceConfig;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * The name of the evaluation.\n   */\n  name: string;\n\n  /**\n   * The object type.\n   */\n  object: 'eval';\n\n  /**\n   * A list of testing criteria.\n   */\n  testing_criteria: Array<\n    | EvalLabelModelGrader\n    | EvalStringCheckGrader\n    | EvalTextSimilarityGrader\n    | EvalRetrieveResponse.Python\n    | EvalRetrieveResponse.ScoreModel\n  >;\n}\n\nexport namespace EvalRetrieveResponse {\n  /**\n   * A PythonGrader object that runs a python script on the input.\n   */\n  export interface Python {\n    /**\n     * The name of the grader.\n     */\n    name: string;\n\n    /**\n     * The source code of the python script.\n     */\n    source: string;\n\n    /**\n     * The object type, which is always `python`.\n     */\n    type: 'python';\n\n    /**\n     * The image tag to use for the python script.\n     */\n    image_tag?: string;\n\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n  }\n\n  /**\n   * A ScoreModelGrader object that uses a model to assign a score to the input.\n   */\n  export interface ScoreModel {\n    /**\n     * The input text. This may include template strings.\n     */\n    input: Array<ScoreModel.Input>;\n\n    /**\n     * The model to use for the evaluation.\n     */\n    model: string;\n\n    /**\n     * The name of the grader.\n     */\n    name: string;\n\n    /**\n     * The object type, which is always `score_model`.\n     */\n    type: 'score_model';\n\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n\n    /**\n     * The range of the score. Defaults to `[0, 1]`.\n     */\n    range?: Array<number>;\n\n    /**\n     * The sampling parameters for the model.\n     */\n    sampling_params?: unknown;\n  }\n\n  export namespace ScoreModel {\n    /**\n     * A message input to the model with a role indicating instruction following\n     * hierarchy. Instructions given with the `developer` or `system` role take\n     * precedence over instructions given with the `user` role. Messages with the\n     * `assistant` role are presumed to have been generated by the model in previous\n     * interactions.\n     */\n    export interface Input {\n      /**\n       * Text inputs to the model - can contain template strings.\n       */\n      content: string | ResponsesAPI.ResponseInputText | Input.OutputText;\n\n      /**\n       * The role of the message input. One of `user`, `assistant`, `system`, or\n       * `developer`.\n       */\n      role: 'user' | 'assistant' | 'system' | 'developer';\n\n      /**\n       * The type of the message input. Always `message`.\n       */\n      type?: 'message';\n    }\n\n    export namespace Input {\n      /**\n       * A text output from the model.\n       */\n      export interface OutputText {\n        /**\n         * The text output from the model.\n         */\n        text: string;\n\n        /**\n         * The type of the output text. Always `output_text`.\n         */\n        type: 'output_text';\n      }\n    }\n  }\n}\n\n/**\n * An Eval object with a data source config and testing criteria. An Eval\n * represents a task to be done for your LLM integration. Like:\n *\n * - Improve the quality of my chatbot\n * - See how well my chatbot handles customer support\n * - Check if o3-mini is better at my usecase than gpt-4o\n */\nexport interface EvalUpdateResponse {\n  /**\n   * Unique identifier for the evaluation.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the eval was created.\n   */\n  created_at: number;\n\n  /**\n   * Configuration of data sources used in runs of the evaluation.\n   */\n  data_source_config: EvalCustomDataSourceConfig | EvalStoredCompletionsDataSourceConfig;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * The name of the evaluation.\n   */\n  name: string;\n\n  /**\n   * The object type.\n   */\n  object: 'eval';\n\n  /**\n   * A list of testing criteria.\n   */\n  testing_criteria: Array<\n    | EvalLabelModelGrader\n    | EvalStringCheckGrader\n    | EvalTextSimilarityGrader\n    | EvalUpdateResponse.Python\n    | EvalUpdateResponse.ScoreModel\n  >;\n}\n\nexport namespace EvalUpdateResponse {\n  /**\n   * A PythonGrader object that runs a python script on the input.\n   */\n  export interface Python {\n    /**\n     * The name of the grader.\n     */\n    name: string;\n\n    /**\n     * The source code of the python script.\n     */\n    source: string;\n\n    /**\n     * The object type, which is always `python`.\n     */\n    type: 'python';\n\n    /**\n     * The image tag to use for the python script.\n     */\n    image_tag?: string;\n\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n  }\n\n  /**\n   * A ScoreModelGrader object that uses a model to assign a score to the input.\n   */\n  export interface ScoreModel {\n    /**\n     * The input text. This may include template strings.\n     */\n    input: Array<ScoreModel.Input>;\n\n    /**\n     * The model to use for the evaluation.\n     */\n    model: string;\n\n    /**\n     * The name of the grader.\n     */\n    name: string;\n\n    /**\n     * The object type, which is always `score_model`.\n     */\n    type: 'score_model';\n\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n\n    /**\n     * The range of the score. Defaults to `[0, 1]`.\n     */\n    range?: Array<number>;\n\n    /**\n     * The sampling parameters for the model.\n     */\n    sampling_params?: unknown;\n  }\n\n  export namespace ScoreModel {\n    /**\n     * A message input to the model with a role indicating instruction following\n     * hierarchy. Instructions given with the `developer` or `system` role take\n     * precedence over instructions given with the `user` role. Messages with the\n     * `assistant` role are presumed to have been generated by the model in previous\n     * interactions.\n     */\n    export interface Input {\n      /**\n       * Text inputs to the model - can contain template strings.\n       */\n      content: string | ResponsesAPI.ResponseInputText | Input.OutputText;\n\n      /**\n       * The role of the message input. One of `user`, `assistant`, `system`, or\n       * `developer`.\n       */\n      role: 'user' | 'assistant' | 'system' | 'developer';\n\n      /**\n       * The type of the message input. Always `message`.\n       */\n      type?: 'message';\n    }\n\n    export namespace Input {\n      /**\n       * A text output from the model.\n       */\n      export interface OutputText {\n        /**\n         * The text output from the model.\n         */\n        text: string;\n\n        /**\n         * The type of the output text. Always `output_text`.\n         */\n        type: 'output_text';\n      }\n    }\n  }\n}\n\n/**\n * An Eval object with a data source config and testing criteria. An Eval\n * represents a task to be done for your LLM integration. Like:\n *\n * - Improve the quality of my chatbot\n * - See how well my chatbot handles customer support\n * - Check if o3-mini is better at my usecase than gpt-4o\n */\nexport interface EvalListResponse {\n  /**\n   * Unique identifier for the evaluation.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the eval was created.\n   */\n  created_at: number;\n\n  /**\n   * Configuration of data sources used in runs of the evaluation.\n   */\n  data_source_config: EvalCustomDataSourceConfig | EvalStoredCompletionsDataSourceConfig;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata: Shared.Metadata | null;\n\n  /**\n   * The name of the evaluation.\n   */\n  name: string;\n\n  /**\n   * The object type.\n   */\n  object: 'eval';\n\n  /**\n   * A list of testing criteria.\n   */\n  testing_criteria: Array<\n    | EvalLabelModelGrader\n    | EvalStringCheckGrader\n    | EvalTextSimilarityGrader\n    | EvalListResponse.Python\n    | EvalListResponse.ScoreModel\n  >;\n}\n\nexport namespace EvalListResponse {\n  /**\n   * A PythonGrader object that runs a python script on the input.\n   */\n  export interface Python {\n    /**\n     * The name of the grader.\n     */\n    name: string;\n\n    /**\n     * The source code of the python script.\n     */\n    source: string;\n\n    /**\n     * The object type, which is always `python`.\n     */\n    type: 'python';\n\n    /**\n     * The image tag to use for the python script.\n     */\n    image_tag?: string;\n\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n  }\n\n  /**\n   * A ScoreModelGrader object that uses a model to assign a score to the input.\n   */\n  export interface ScoreModel {\n    /**\n     * The input text. This may include template strings.\n     */\n    input: Array<ScoreModel.Input>;\n\n    /**\n     * The model to use for the evaluation.\n     */\n    model: string;\n\n    /**\n     * The name of the grader.\n     */\n    name: string;\n\n    /**\n     * The object type, which is always `score_model`.\n     */\n    type: 'score_model';\n\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n\n    /**\n     * The range of the score. Defaults to `[0, 1]`.\n     */\n    range?: Array<number>;\n\n    /**\n     * The sampling parameters for the model.\n     */\n    sampling_params?: unknown;\n  }\n\n  export namespace ScoreModel {\n    /**\n     * A message input to the model with a role indicating instruction following\n     * hierarchy. Instructions given with the `developer` or `system` role take\n     * precedence over instructions given with the `user` role. Messages with the\n     * `assistant` role are presumed to have been generated by the model in previous\n     * interactions.\n     */\n    export interface Input {\n      /**\n       * Text inputs to the model - can contain template strings.\n       */\n      content: string | ResponsesAPI.ResponseInputText | Input.OutputText;\n\n      /**\n       * The role of the message input. One of `user`, `assistant`, `system`, or\n       * `developer`.\n       */\n      role: 'user' | 'assistant' | 'system' | 'developer';\n\n      /**\n       * The type of the message input. Always `message`.\n       */\n      type?: 'message';\n    }\n\n    export namespace Input {\n      /**\n       * A text output from the model.\n       */\n      export interface OutputText {\n        /**\n         * The text output from the model.\n         */\n        text: string;\n\n        /**\n         * The type of the output text. Always `output_text`.\n         */\n        type: 'output_text';\n      }\n    }\n  }\n}\n\nexport interface EvalDeleteResponse {\n  deleted: boolean;\n\n  eval_id: string;\n\n  object: string;\n}\n\nexport interface EvalCreateParams {\n  /**\n   * The configuration for the data source used for the evaluation runs.\n   */\n  data_source_config: EvalCreateParams.Custom | EvalCreateParams.Logs;\n\n  /**\n   * A list of graders for all eval runs in this group.\n   */\n  testing_criteria: Array<\n    | EvalCreateParams.LabelModel\n    | EvalStringCheckGrader\n    | EvalTextSimilarityGrader\n    | EvalCreateParams.Python\n    | EvalCreateParams.ScoreModel\n  >;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * The name of the evaluation.\n   */\n  name?: string;\n}\n\nexport namespace EvalCreateParams {\n  /**\n   * A CustomDataSourceConfig object that defines the schema for the data source used\n   * for the evaluation runs. This schema is used to define the shape of the data\n   * that will be:\n   *\n   * - Used to define your testing criteria and\n   * - What data is required when creating a run\n   */\n  export interface Custom {\n    /**\n     * The json schema for each row in the data source.\n     */\n    item_schema: Record<string, unknown>;\n\n    /**\n     * The type of data source. Always `custom`.\n     */\n    type: 'custom';\n\n    /**\n     * Whether the eval should expect you to populate the sample namespace (ie, by\n     * generating responses off of your data source)\n     */\n    include_sample_schema?: boolean;\n  }\n\n  /**\n   * A data source config which specifies the metadata property of your stored\n   * completions query. This is usually metadata like `usecase=chatbot` or\n   * `prompt-version=v2`, etc.\n   */\n  export interface Logs {\n    /**\n     * The type of data source. Always `logs`.\n     */\n    type: 'logs';\n\n    /**\n     * Metadata filters for the logs data source.\n     */\n    metadata?: Record<string, unknown>;\n  }\n\n  /**\n   * A LabelModelGrader object which uses a model to assign labels to each item in\n   * the evaluation.\n   */\n  export interface LabelModel {\n    /**\n     * A list of chat messages forming the prompt or context. May include variable\n     * references to the \"item\" namespace, ie {{item.name}}.\n     */\n    input: Array<LabelModel.SimpleInputMessage | LabelModel.EvalItem>;\n\n    /**\n     * The labels to classify to each item in the evaluation.\n     */\n    labels: Array<string>;\n\n    /**\n     * The model to use for the evaluation. Must support structured outputs.\n     */\n    model: string;\n\n    /**\n     * The name of the grader.\n     */\n    name: string;\n\n    /**\n     * The labels that indicate a passing result. Must be a subset of labels.\n     */\n    passing_labels: Array<string>;\n\n    /**\n     * The object type, which is always `label_model`.\n     */\n    type: 'label_model';\n  }\n\n  export namespace LabelModel {\n    export interface SimpleInputMessage {\n      /**\n       * The content of the message.\n       */\n      content: string;\n\n      /**\n       * The role of the message (e.g. \"system\", \"assistant\", \"user\").\n       */\n      role: string;\n    }\n\n    /**\n     * A message input to the model with a role indicating instruction following\n     * hierarchy. Instructions given with the `developer` or `system` role take\n     * precedence over instructions given with the `user` role. Messages with the\n     * `assistant` role are presumed to have been generated by the model in previous\n     * interactions.\n     */\n    export interface EvalItem {\n      /**\n       * Text inputs to the model - can contain template strings.\n       */\n      content: string | ResponsesAPI.ResponseInputText | EvalItem.OutputText;\n\n      /**\n       * The role of the message input. One of `user`, `assistant`, `system`, or\n       * `developer`.\n       */\n      role: 'user' | 'assistant' | 'system' | 'developer';\n\n      /**\n       * The type of the message input. Always `message`.\n       */\n      type?: 'message';\n    }\n\n    export namespace EvalItem {\n      /**\n       * A text output from the model.\n       */\n      export interface OutputText {\n        /**\n         * The text output from the model.\n         */\n        text: string;\n\n        /**\n         * The type of the output text. Always `output_text`.\n         */\n        type: 'output_text';\n      }\n    }\n  }\n\n  /**\n   * A PythonGrader object that runs a python script on the input.\n   */\n  export interface Python {\n    /**\n     * The name of the grader.\n     */\n    name: string;\n\n    /**\n     * The source code of the python script.\n     */\n    source: string;\n\n    /**\n     * The object type, which is always `python`.\n     */\n    type: 'python';\n\n    /**\n     * The image tag to use for the python script.\n     */\n    image_tag?: string;\n\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n  }\n\n  /**\n   * A ScoreModelGrader object that uses a model to assign a score to the input.\n   */\n  export interface ScoreModel {\n    /**\n     * The input text. This may include template strings.\n     */\n    input: Array<ScoreModel.Input>;\n\n    /**\n     * The model to use for the evaluation.\n     */\n    model: string;\n\n    /**\n     * The name of the grader.\n     */\n    name: string;\n\n    /**\n     * The object type, which is always `score_model`.\n     */\n    type: 'score_model';\n\n    /**\n     * The threshold for the score.\n     */\n    pass_threshold?: number;\n\n    /**\n     * The range of the score. Defaults to `[0, 1]`.\n     */\n    range?: Array<number>;\n\n    /**\n     * The sampling parameters for the model.\n     */\n    sampling_params?: unknown;\n  }\n\n  export namespace ScoreModel {\n    /**\n     * A message input to the model with a role indicating instruction following\n     * hierarchy. Instructions given with the `developer` or `system` role take\n     * precedence over instructions given with the `user` role. Messages with the\n     * `assistant` role are presumed to have been generated by the model in previous\n     * interactions.\n     */\n    export interface Input {\n      /**\n       * Text inputs to the model - can contain template strings.\n       */\n      content: string | ResponsesAPI.ResponseInputText | Input.OutputText;\n\n      /**\n       * The role of the message input. One of `user`, `assistant`, `system`, or\n       * `developer`.\n       */\n      role: 'user' | 'assistant' | 'system' | 'developer';\n\n      /**\n       * The type of the message input. Always `message`.\n       */\n      type?: 'message';\n    }\n\n    export namespace Input {\n      /**\n       * A text output from the model.\n       */\n      export interface OutputText {\n        /**\n         * The text output from the model.\n         */\n        text: string;\n\n        /**\n         * The type of the output text. Always `output_text`.\n         */\n        type: 'output_text';\n      }\n    }\n  }\n}\n\nexport interface EvalUpdateParams {\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard.\n   *\n   * Keys are strings with a maximum length of 64 characters. Values are strings with\n   * a maximum length of 512 characters.\n   */\n  metadata?: Shared.Metadata | null;\n\n  /**\n   * Rename the evaluation.\n   */\n  name?: string;\n}\n\nexport interface EvalListParams extends CursorPageParams {\n  /**\n   * Sort order for evals by timestamp. Use `asc` for ascending order or `desc` for\n   * descending order.\n   */\n  order?: 'asc' | 'desc';\n\n  /**\n   * Evals can be ordered by creation time or last updated time. Use `created_at` for\n   * creation time or `updated_at` for last updated time.\n   */\n  order_by?: 'created_at' | 'updated_at';\n}\n\nEvals.EvalListResponsesPage = EvalListResponsesPage;\nEvals.Runs = Runs;\nEvals.RunListResponsesPage = RunListResponsesPage;\n\nexport declare namespace Evals {\n  export {\n    type EvalCustomDataSourceConfig as EvalCustomDataSourceConfig,\n    type EvalLabelModelGrader as EvalLabelModelGrader,\n    type EvalStoredCompletionsDataSourceConfig as EvalStoredCompletionsDataSourceConfig,\n    type EvalStringCheckGrader as EvalStringCheckGrader,\n    type EvalTextSimilarityGrader as EvalTextSimilarityGrader,\n    type EvalCreateResponse as EvalCreateResponse,\n    type EvalRetrieveResponse as EvalRetrieveResponse,\n    type EvalUpdateResponse as EvalUpdateResponse,\n    type EvalListResponse as EvalListResponse,\n    type EvalDeleteResponse as EvalDeleteResponse,\n    EvalListResponsesPage as EvalListResponsesPage,\n    type EvalCreateParams as EvalCreateParams,\n    type EvalUpdateParams as EvalUpdateParams,\n    type EvalListParams as EvalListParams,\n  };\n\n  export {\n    Runs as Runs,\n    type CreateEvalCompletionsRunDataSource as CreateEvalCompletionsRunDataSource,\n    type CreateEvalJSONLRunDataSource as CreateEvalJSONLRunDataSource,\n    type EvalAPIError as EvalAPIError,\n    type RunCreateResponse as RunCreateResponse,\n    type RunRetrieveResponse as RunRetrieveResponse,\n    type RunListResponse as RunListResponse,\n    type RunDeleteResponse as RunDeleteResponse,\n    type RunCancelResponse as RunCancelResponse,\n    RunListResponsesPage as RunListResponsesPage,\n    type RunCreateParams as RunCreateParams,\n    type RunListParams as RunListParams,\n  };\n}\n"],"mappings":"AAAA;SAESA,WAAW,QAAE;SACbC,gBAAgB,QAAE;OAIpB,KAAKC,OAAO;SAWjBC,oBAAoB,EAEpBC,IAAI,QACL;SACQC,UAAU,QAAyB;AAE5C,OAAM,MAAOC,KAAM,SAAQN,WAAW;EAAtCO,YAAA;;IACE,KAAAC,IAAI,GAAiB,IAAIN,OAAO,CAACE,IAAI,CAAC,IAAI,CAACK,OAAO,CAAC;EAuDrD;EArDE;;;;;;;EAOAC,MAAMA,CAACC,IAAsB,EAAEC,OAA6B;IAC1D,OAAO,IAAI,CAACH,OAAO,CAACI,IAAI,CAAC,QAAQ,EAAE;MAAEF,IAAI;MAAE,GAAGC;IAAO,CAAE,CAAC;EAC1D;EAEA;;;EAGAE,QAAQA,CAACC,MAAc,EAAEH,OAA6B;IACpD,OAAO,IAAI,CAACH,OAAO,CAACO,GAAG,CAAC,UAAUD,MAAM,EAAE,EAAEH,OAAO,CAAC;EACtD;EAEA;;;EAGAK,MAAMA,CACJF,MAAc,EACdJ,IAAsB,EACtBC,OAA6B;IAE7B,OAAO,IAAI,CAACH,OAAO,CAACI,IAAI,CAAC,UAAUE,MAAM,EAAE,EAAE;MAAEJ,IAAI;MAAE,GAAGC;IAAO,CAAE,CAAC;EACpE;EAUAM,IAAIA,CACFC,KAAA,GAA8C,EAAE,EAChDP,OAA6B;IAE7B,IAAIX,gBAAgB,CAACkB,KAAK,CAAC,EAAE;MAC3B,OAAO,IAAI,CAACD,IAAI,CAAC,EAAE,EAAEC,KAAK,CAAC;;IAE7B,OAAO,IAAI,CAACV,OAAO,CAACW,UAAU,CAAC,QAAQ,EAAEC,qBAAqB,EAAE;MAAEF,KAAK;MAAE,GAAGP;IAAO,CAAE,CAAC;EACxF;EAEA;;;EAGAU,GAAGA,CAACP,MAAc,EAAEH,OAA6B;IAC/C,OAAO,IAAI,CAACH,OAAO,CAACc,MAAM,CAAC,UAAUR,MAAM,EAAE,EAAEH,OAAO,CAAC;EACzD;;AAGF,OAAM,MAAOS,qBAAsB,SAAQhB,UAA4B;AAmsCvEC,KAAK,CAACe,qBAAqB,GAAGA,qBAAqB;AACnDf,KAAK,CAACF,IAAI,GAAGA,IAAI;AACjBE,KAAK,CAACH,oBAAoB,GAAGA,oBAAoB","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}